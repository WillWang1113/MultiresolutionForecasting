{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. No adjusts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electricity load dataset (persistence: loop)\n",
    "# pth = \"results/mfred/example_benchmarks-20230831-041824\"\n",
    "\n",
    "# wind power dataset (persistence: naive)\n",
    "pth = \"results/nrel/example_benchmarks-20230817-231811\"\n",
    "# pth = \"results/mfred/example_benchmarks-20230814-210128\"\n",
    "\n",
    "# NL with one large Laplace decoder (failure)\n",
    "# pth = \"results/mfred/example_benchmarks-20231012-175829\"\n",
    "\n",
    "# model_names = ['Neural Laplace', 'LSTM', 'MLP']\n",
    "model_names = ['Neural Laplace', 'LSTM', 'MLP', 'Persistence']\n",
    "# model_names = ['Neural Laplace']\n",
    "fcst_features = [0]\n",
    "all_seed_result = []\n",
    "all_seed_preds = []\n",
    "for seed in range(20):\n",
    "    with open(f\"{pth}-{seed}.pkl\", \"rb\") as f:\n",
    "        all_result = pickle.load(f)\n",
    "    test_result = {name: {} for name in model_names}\n",
    "    test_preds_trajs = {name: {} for name in model_names}\n",
    "    for avg_terms in all_result:\n",
    "        result_avg = all_result[avg_terms]\n",
    "        train_mean = result_avg['train_mean'][fcst_features]\n",
    "        train_std = result_avg['train_std'][fcst_features]\n",
    "        num_avg_terms = int(avg_terms.split(\"_\")[-1])\n",
    "        for name in model_names:\n",
    "            model_results = result_avg[name]\n",
    "\n",
    "            test_preds = model_results[\"test_preds\"] * train_std.cpu().numpy(\n",
    "            ) + train_mean.cpu().numpy()\n",
    "            test_trajs = model_results[\"test_trajs\"] * train_std.cpu().numpy(\n",
    "            ) + train_mean.cpu().numpy()\n",
    "            pred_timesteps = test_trajs.shape[1]\n",
    "            # if num_avg_terms == 1:\n",
    "            #     fig, ax = plt.subplots()\n",
    "            #     ax.plot(test_trajs[300,:,0], label=\"real\")\n",
    "            #     ax.plot(test_preds[300,:,0], label=\"fcst\")\n",
    "            #     ax.legend()\n",
    "            #     ax.set(xlabel=\"Forecasting horizon\", ylabel=\"Load(kW)\")\n",
    "            #     # ax.set_title(name)\n",
    "            #     fig.savefig(\"savings/supp_NL.pdf\")\n",
    "            test_result[name][num_avg_terms] = mean_squared_error(\n",
    "                test_trajs.squeeze(), test_preds.squeeze(), squared=False)\n",
    "            test_preds_trajs[name][num_avg_terms] = deepcopy(\n",
    "                (test_preds, test_trajs))\n",
    "    df_test_result = pd.DataFrame(test_result)\n",
    "    all_seed_preds.append(test_preds_trajs)\n",
    "    all_seed_result.append(df_test_result)\n",
    "all_seed_result = pd.concat(all_seed_result, axis=0)\n",
    "all_seed_result = all_seed_result.groupby(all_seed_result.index).agg(\n",
    "    [\"mean\", \"std\"])\n",
    "all_seed_result = all_seed_result.sort_index()\n",
    "avg_terms_list = all_seed_result.index.tolist()\n",
    "all_seed_result\n",
    "# with open(f'savings/bench_20_{pth.split(\"/\")[1]}.pickle', 'wb') as handle:\n",
    "#     pickle.dump(all_seed_preds, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean consistent error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_avg_terms = max(avg_terms_list)\n",
    "mce_results = {name: [] for name in model_names}\n",
    "for seed in range(20):\n",
    "    preds_trajs_dict = all_seed_preds[seed]\n",
    "    for name in model_names:\n",
    "        mce = {\"5min vs 15min\":0, \"5min vs 60min\":0, \"15min vs 60min\":0}\n",
    "        for n, avg_terms in enumerate(avg_terms_list):\n",
    "\n",
    "            if avg_terms < max_avg_terms:\n",
    "                # print(\"current avg_terms\", avg_terms)\n",
    "                rest_avg_terms = avg_terms_list[n + 1:]\n",
    "                (test_preds, test_trajs) = preds_trajs_dict[name][avg_terms]\n",
    "                for r_avg_terms in rest_avg_terms:\n",
    "                    (real_avg_preds,\n",
    "                     real_avg_trajs) = preds_trajs_dict[name][r_avg_terms]\n",
    "                    # print(avg_terms, r_avg_terms)\n",
    "                    # calculate predictions\n",
    "                    avg_test_preds = np.split(test_preds,\n",
    "                                              test_preds.shape[1] //\n",
    "                                              (r_avg_terms // avg_terms),\n",
    "                                              axis=1)\n",
    "                    avg_test_preds = np.stack(\n",
    "                        [j.mean(axis=1) for j in avg_test_preds], axis=1)\n",
    "                    error = (avg_test_preds - real_avg_preds)**2\n",
    "                    mce[f\"{avg_terms*5}min vs {r_avg_terms*5}min\"] += error.mean()\n",
    "        # mce = pd.DataFrame(mce, index=0)\n",
    "        # print(mce)\n",
    "        mce_results[name].append(mce)\n",
    "\n",
    "all_model_mce = []\n",
    "for name in model_names:\n",
    "    \n",
    "    model_mce = pd.DataFrame(mce_results[name]).mean()\n",
    "    all_model_mce.append(model_mce)\n",
    "all_model_mce = pd.concat(all_model_mce, axis=1)\n",
    "all_model_mce = all_model_mce.T\n",
    "all_model_mce.index = model_names\n",
    "all_model_mce = all_model_mce.drop(\"Persistence\", axis=0)\n",
    "all_model_mce\n",
    "# mce_results.index = [0 for _ in range(20)]\n",
    "# mce_results = mce_results.groupby(mce_results.index).agg(\n",
    "    # [\"mean\", \"std\"])\n",
    "# mce_results.index= [\"MCE\"]\n",
    "# mce_results.mean()\n",
    "# mce_results.to_csv(f\"savings/bench_{pth.split('/')[1]}_mce.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. After adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Bottom up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bu_results, all_bu_preds = [], []\n",
    "for seed in range(20):\n",
    "    preds_trajs_dict = all_seed_preds[seed]\n",
    "    bu_results = {name + \"-BU\": {} for name in model_names}\n",
    "    bu_preds = {name + \"-BU\": {} for name in model_names}\n",
    "    for name in model_names:\n",
    "        for avg_terms in avg_terms_list:\n",
    "            (test_preds, test_trajs) = preds_trajs_dict[name][1]\n",
    "            # Test trajs are consistent\n",
    "            avg_test_trajs = np.split(test_trajs,\n",
    "                                      test_trajs.shape[1] // avg_terms,\n",
    "                                      axis=1)\n",
    "            avg_test_trajs = np.stack([j.mean(axis=1) for j in avg_test_trajs],\n",
    "                                      axis=1)\n",
    "\n",
    "            # calculate predictions\n",
    "            avg_test_preds = np.split(test_preds,\n",
    "                                      test_preds.shape[1] // avg_terms,\n",
    "                                      axis=1)\n",
    "            avg_test_preds = np.stack([j.mean(axis=1) for j in avg_test_preds],\n",
    "                                      axis=1)\n",
    "            bu_results[name + \"-BU\"][avg_terms] = mean_squared_error(\n",
    "                avg_test_trajs.flatten(),\n",
    "                avg_test_preds.flatten(),\n",
    "                squared=False)\n",
    "            bu_preds[name + \"-BU\"][avg_terms] = (avg_test_preds,avg_test_trajs\n",
    "                                                 )\n",
    "\n",
    "            # if avg_terms == 3:\n",
    "            # fig, ax = plt.subplots()\n",
    "            # ax.plot(avg_test_trajs[0,:,0])\n",
    "            # ax.plot(avg_test_preds[0,:,0])\n",
    "\n",
    "    bu_results = pd.DataFrame(bu_results)\n",
    "    all_bu_results.append(bu_results)\n",
    "    all_bu_preds.append(bu_preds)\n",
    "all_bu_results = pd.concat(all_bu_results, axis=0)\n",
    "all_bu_results = all_bu_results.groupby(all_bu_results.index).agg(\n",
    "    [\"mean\", \"std\"])\n",
    "all_bu_results = all_bu_results.sort_index()\n",
    "all_bu_results\n",
    "# with open(f'savings/bench_bu_20_{pth.split(\"/\")[1]}.pickle', 'wb') as handle:\n",
    "#     pickle.dump(all_bu_preds, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# all_bu_results.to_csv(\"savings/benchmark_bu.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Athanasopoulos, George, et al. \"Forecasting with temporal hierarchies.\" European Journal of Operational Research 262.1 (2017): 60-74."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import block_diag\n",
    "\n",
    "S = [\n",
    "    block_diag(*[[1.0 / s for _ in range(s)]\n",
    "                 for _ in range(pred_timesteps // s)]) for s in avg_terms_list\n",
    "]\n",
    "S.reverse()\n",
    "S = np.concatenate(S)\n",
    "S = S @ np.linalg.inv(S.T @ S) @ S.T\n",
    "\n",
    "all_opt_results, all_opt_preds = [], []\n",
    "for seed in range(20):\n",
    "    preds_trajs_dict = all_seed_preds[seed]\n",
    "    opt_results = {name + \"-OPT\": {} for name in model_names}\n",
    "    opt_preds = {name + \"-OPT\": {} for name in model_names}\n",
    "    for name in model_names:\n",
    "        preds_vec, trajs_vec, pred_steps = [], [], []\n",
    "        for avg_terms in avg_terms_list[::-1]:\n",
    "            (test_preds, test_trajs) = preds_trajs_dict[name][avg_terms]\n",
    "            preds_vec.append(test_preds.copy())\n",
    "            trajs_vec.append(test_trajs.copy())\n",
    "        preds_vec = np.concatenate(preds_vec, axis=1)\n",
    "        # preds_vec = preds_vec.transpose(1, 0, 2)\n",
    "        adjusted_preds = np.tensordot(S, preds_vec, axes=[1, 1])\n",
    "        start = 0\n",
    "        for i, trajs in enumerate(trajs_vec):\n",
    "            steps = trajs.shape[1]\n",
    "            opt_results[name +\n",
    "                        \"-OPT\"][avg_terms_list[::-1][i]] = mean_squared_error(\n",
    "                            trajs.flatten(),\n",
    "                            adjusted_preds[start:start + steps,\n",
    "                                           ...].transpose(1, 0, 2).flatten(),\n",
    "                            squared=False)\n",
    "            opt_preds[name + \"-OPT\"][avg_terms_list[::-1][i]] = (\n",
    "                adjusted_preds[start:start + steps, ...].transpose(1, 0,\n",
    "                                                                   2), trajs)\n",
    "            # if i == 0:\n",
    "            #     fig, ax = plt.subplots()\n",
    "            #     ax.plot(trajs[0].flatten())\n",
    "            #     ax.plot(adjusted_preds[start:start + steps, 0, :].flatten())\n",
    "            #     print(adjusted_preds[start:start + steps, 0, :].flatten())\n",
    "            start += steps\n",
    "\n",
    "    opt_results = pd.DataFrame(opt_results)\n",
    "    all_opt_results.append(opt_results)\n",
    "    all_opt_preds.append(opt_preds)\n",
    "\n",
    "all_opt_results = pd.concat(all_opt_results, axis=0)\n",
    "all_opt_results = all_opt_results.groupby(all_opt_results.index).agg(\n",
    "    [\"mean\", \"std\"])\n",
    "all_opt_results = all_opt_results.sort_index()\n",
    "# all_opt_results.to_csv(\"savings/benchmark_opt.csv\")\n",
    "# with open(f'savings/bench_opt_20_{pth.split(\"/\")[1]}.pickle', 'wb') as handle:\n",
    "#     pickle.dump(all_opt_preds, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "all_opt_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical NL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. No adjusts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load\n",
    "pth = \"results/mfred/example-20230815-052223\"\n",
    "\n",
    "# Wind\n",
    "# pth = \"results/nrel/example-20230819-070441\"\n",
    "\n",
    "\n",
    "all_seed_result = []\n",
    "all_seed_preds = []\n",
    "fcst_features = [0]\n",
    "for seed in range(20):\n",
    "    with open(f\"{pth}-{seed}.pkl\", \"rb\") as f:\n",
    "        result = pickle.load(f)\n",
    "        avg_terms_list = result[\"model_hyperparams\"][\"avg_terms_list\"]\n",
    "        # print(result[\"model_hyperparams\"][\"encoder\"])\n",
    "        train_mean = result[\"train_mean\"][fcst_features].detach().cpu().numpy()\n",
    "        train_std = result[\"train_std\"][fcst_features].detach().cpu().numpy()\n",
    "\n",
    "        model = result[\"Hierarchical NL\"]\n",
    "        print(model[\"train_losses\"][:10])\n",
    "        # avg_terms_list = result[\"avg_terms_list\"]\n",
    "        # print(result[\"train_mean\"])\n",
    "        # print(result[\"train_std\"])\n",
    "        test_label = model[\"test_trajs\"]\n",
    "        test_preds = model[\"test_preds\"]\n",
    "        test_label = test_label\n",
    "        test_label = test_label * train_std + train_mean\n",
    "        pred_timesteps = test_label.shape[1]\n",
    "\n",
    "    avg_test_label, avg_test_preds, rmse = {}, {}, {}\n",
    "    for i, avg_terms in enumerate(avg_terms_list):\n",
    "        temp = np.split(test_label, test_label.shape[1] // avg_terms, axis=1)\n",
    "        temp = np.stack([j.mean(axis=1) for j in temp], axis=1)\n",
    "        avg_test_label[avg_terms] = temp\n",
    "        # avg_test_preds[avg_terms] = (test_preds[i].detach().cpu().numpy(\n",
    "        # ), temp)\n",
    "        avg_test_preds[avg_terms] = (\n",
    "            test_preds[i].detach().cpu().numpy() * train_std + train_mean,\n",
    "            temp)\n",
    "\n",
    "        # print(avg_test_preds[avg_terms][0].flatten())\n",
    "        rmse[avg_terms] = [\n",
    "            mean_squared_error(temp.squeeze(),\n",
    "                               avg_test_preds[avg_terms][0].squeeze(),\n",
    "                               squared=False)\n",
    "        ]\n",
    "        # fig, ax = plt.subplots()\n",
    "        # ax.plot(temp[3,:,0])\n",
    "        # ax.plot(avg_test_preds[avg_terms][0][3,:,0])\n",
    "    rmse = pd.DataFrame(rmse).transpose()\n",
    "    all_seed_result.append(rmse)\n",
    "    all_seed_preds.append(avg_test_preds)\n",
    "all_seed_result = pd.concat(all_seed_result, axis=0)\n",
    "all_seed_result = all_seed_result.groupby(all_seed_result.index).agg(\n",
    "    [\"mean\", \"std\"])\n",
    "avg_terms_list = all_seed_result.index.tolist()\n",
    "all_seed_result = all_seed_result.rename(columns={0: \"Hierarchical NL\"})\n",
    "all_seed_result\n",
    "\n",
    "# with open(f'savings/proposed_20_{pth.split(\"/\")[1]}.pickle', 'wb') as handle:\n",
    "#     pickle.dump(all_seed_preds, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean consistence error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_avg_terms = max(avg_terms_list)\n",
    "mce_results = {}\n",
    "for seed in range(20):\n",
    "    preds_trajs_dict = all_seed_preds[seed]\n",
    "    mce = {\"5min vs 15min\":0, \"5min vs 60min\":0, \"15min vs 60min\":0}\n",
    "    for n, avg_terms in enumerate(avg_terms_list):\n",
    "\n",
    "        if avg_terms < max_avg_terms:\n",
    "            # print(\"current avg_terms\", avg_terms)\n",
    "            rest_avg_terms = avg_terms_list[n + 1:]\n",
    "            test_preds, _ = preds_trajs_dict[avg_terms]\n",
    "            for r_avg_terms in rest_avg_terms:\n",
    "                real_avg_preds, _ = preds_trajs_dict[r_avg_terms]\n",
    "\n",
    "                # print(avg_terms, r_avg_terms)\n",
    "                # calculate predictions\n",
    "                avg_test_preds = np.split(test_preds,\n",
    "                                          test_preds.shape[1] //\n",
    "                                          (r_avg_terms // avg_terms),\n",
    "                                          axis=1)\n",
    "                avg_test_preds = np.stack(\n",
    "                    [j.mean(axis=1) for j in avg_test_preds], axis=1)\n",
    "                error = (avg_test_preds - real_avg_preds)**2\n",
    "                mce[f\"{avg_terms*5}min vs {r_avg_terms*5}min\"] += error.mean()\n",
    "    mce_results[seed] = mce\n",
    "    # mce_results[seed] = [mce]\n",
    "mce_results = pd.DataFrame(mce_results)\n",
    "mce_results.index.name = \"seed\"\n",
    "mce_results = mce_results.transpose()\n",
    "print(mce_results)\n",
    "# mce_results.columns=[\"Hierarchical NL\"]\n",
    "# mce_results.index = [0 for _ in range(20)]\n",
    "mce_results = mce_results.mean()\n",
    "# mce_results.index= [\"MCE\"]\n",
    "all_model_mce = pd.concat([all_model_mce.T, mce_results], axis=1)\n",
    "all_model_mce.rename(columns={0:\"HL\"}, inplace=True)\n",
    "all_model_mce\n",
    "# hnl = pd.concat([all_seed_result, mce_results])\n",
    "# benchmarks = pd.read_csv(\"savings/benchmark_raw.csv\", index_col=0, header=[0,1])\n",
    "# benchmarks.index = hnl.index\n",
    "# all_results = pd.concat([benchmarks, hnl], axis=1)\n",
    "# print(all_results)\n",
    "# mce_results.to_csv(f\"savings/proposed_{pth.split('/')[1]}_mce.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_mce = all_model_mce.sort_values(by = \"15min vs 60min\", axis=1)\n",
    "all_model_mce = all_model_mce.rename(columns={\"Neural Laplace\":\"NL\"})\n",
    "all_model_mce.to_csv(\"wind_mce.csv\")\n",
    "colors = plt.cm.BuPu(np.linspace(0.1, 0.5, len(all_model_mce)))\n",
    "n_rows = len(all_model_mce)\n",
    "\n",
    "index = np.arange(len(all_model_mce.columns))\n",
    "bar_width = 0.6\n",
    "\n",
    "# Initialize the vertical-offset for the stacked bar chart.\n",
    "y_offset = np.zeros(len(all_model_mce.columns))\n",
    "\n",
    "fig, ax=plt.subplots()\n",
    "# Plot bars and create text labels for the table\n",
    "cell_text = []\n",
    "for row in range(n_rows):\n",
    "    ax.bar(all_model_mce.columns, all_model_mce.iloc[row, :], bar_width, bottom=y_offset, color=colors[row], label=all_model_mce.index[row])\n",
    "    y_offset = y_offset + all_model_mce.iloc[row, :]\n",
    "\n",
    "ax.legend(fontsize=12)\n",
    "for i, tick in enumerate(ax.get_xticklabels()):\n",
    "    tick.set_fontsize(14)\n",
    "\n",
    "    if i == 0:\n",
    "        tick.set_fontweight('bold')\n",
    "        tick.set_fontsize(16)\n",
    "\n",
    "for i, tick in enumerate(ax.get_yticklabels()):\n",
    "    tick.set_fontsize(12)\n",
    "ax.ticklabel_format(style='sci', scilimits=(-1,2), axis='y')\n",
    "# fig.savefig(\"savings/mce_wind.pdf\",bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_mce = pd.read_csv(\"savings/load_mce.csv\", index_col=0)\n",
    "wind_mce = pd.read_csv(\"savings/wind_mce.csv\", index_col=0)\n",
    "\n",
    "colors = plt.cm.BuPu(np.linspace(0.1, 0.5, len(wind_mce)))\n",
    "n_rows = len(wind_mce)\n",
    "\n",
    "index = np.arange(len(wind_mce.columns))\n",
    "bar_width = 0.6\n",
    "\n",
    "# Initialize the vertical-offset for the stacked bar chart.\n",
    "y_offset_load = np.zeros(len(load_mce.columns))\n",
    "y_offset_wind = np.zeros(len(wind_mce.columns))\n",
    "\n",
    "fig, axs=plt.subplots(1,2,figsize=[14,6])\n",
    "# Plot bars and create text labels for the table\n",
    "cell_text = []\n",
    "for row in range(n_rows):\n",
    "    axs[0].bar(load_mce.columns, load_mce.iloc[row, :], bar_width, bottom=y_offset_load, color=colors[row], label=load_mce.index[row])\n",
    "    axs[1].bar(wind_mce.columns, wind_mce.iloc[row, :], bar_width, bottom=y_offset_wind, color=colors[row], label=wind_mce.index[row])\n",
    "    y_offset_load = y_offset_load + load_mce.iloc[row, :]\n",
    "    y_offset_wind = y_offset_wind + wind_mce.iloc[row, :]\n",
    "\n",
    "for n in range(2):\n",
    "    for i, tick in enumerate(axs[n].get_xticklabels()):\n",
    "        tick.set_fontsize(14)\n",
    "\n",
    "        if i == 0:\n",
    "            tick.set_fontweight('bold')\n",
    "            tick.set_fontsize(16)\n",
    "\n",
    "    for i, tick in enumerate(axs[n].get_yticklabels()):\n",
    "        tick.set_fontsize(12)\n",
    "    axs[n].ticklabel_format(style='sci', scilimits=(-1,2), axis='y')\n",
    "fig.tight_layout()\n",
    "axs[0].legend(fontsize=\"x-large\",ncol=3,\n",
    "    loc='center',\n",
    "    bbox_to_anchor=(1.05, 1.075),\n",
    "    fancybox=True,)\n",
    "# fig.savefig(\"savings/mce_new.pdf\",bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. After adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 BU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bu_results = []\n",
    "for seed in range(20):\n",
    "    preds_trajs_dict = all_seed_preds[seed]\n",
    "    bu_results = {}\n",
    "\n",
    "    for avg_terms in avg_terms_list:\n",
    "        (test_preds, test_trajs) = preds_trajs_dict[1]\n",
    "        # Test trajs are consistent\n",
    "        avg_test_trajs = np.split(test_trajs,\n",
    "                                    test_trajs.shape[1] // avg_terms,\n",
    "                                    axis=1)\n",
    "        avg_test_trajs = np.stack([j.mean(axis=1) for j in avg_test_trajs],\n",
    "                                    axis=1)\n",
    "\n",
    "        # calculate predictions\n",
    "        avg_test_preds = np.split(test_preds,\n",
    "                                    test_preds.shape[1] // avg_terms,\n",
    "                                    axis=1)\n",
    "        avg_test_preds = np.stack([j.mean(axis=1) for j in avg_test_preds],\n",
    "                                    axis=1)\n",
    "        bu_results[avg_terms] = [mean_squared_error(\n",
    "            avg_test_trajs.flatten(),\n",
    "            avg_test_preds.flatten(),\n",
    "            squared=False)]\n",
    "            # if avg_terms == 3:\n",
    "            # fig, ax = plt.subplots()\n",
    "            # ax.plot(avg_test_trajs[0,:,0])\n",
    "            # ax.plot(avg_test_preds[0,:,0])\n",
    "            \n",
    "    bu_results = pd.DataFrame(bu_results)\n",
    "    all_bu_results.append(bu_results)\n",
    "all_bu_results = pd.concat(all_bu_results, axis=0)\n",
    "all_bu_results = all_bu_results.groupby(all_bu_results.index).agg(\n",
    "    [\"mean\", \"std\"])\n",
    "all_bu_results = all_bu_results.sort_index()\n",
    "all_bu_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import block_diag\n",
    "\n",
    "S = [\n",
    "    block_diag(*[[1.0 / s for _ in range(s)] for _ in range(pred_timesteps // s)])\n",
    "    for s in avg_terms_list\n",
    "]\n",
    "S.reverse()\n",
    "S = np.concatenate(S)\n",
    "S = S @ np.linalg.inv(S.T @ S) @ S.T\n",
    "\n",
    "all_opt_results, all_opt_preds = [], []\n",
    "avg_terms_list.sort(reverse=True)\n",
    "for seed in range(20):\n",
    "    preds_trajs_dict = all_seed_preds[seed]\n",
    "    opt_results = {}\n",
    "    preds_vec, trajs_vec, pred_steps = [], [], []\n",
    "    for avg_terms in avg_terms_list:\n",
    "        (test_preds, test_trajs) = preds_trajs_dict[avg_terms]\n",
    "        preds_vec.append(test_preds.copy())\n",
    "        trajs_vec.append(test_trajs.copy())\n",
    "    preds_vec = np.concatenate(preds_vec, axis=1)\n",
    "    # preds_vec = preds_vec.transpose(1, 0, 2)\n",
    "    adjusted_preds = np.tensordot(S, preds_vec, axes=[1, 1])\n",
    "    start = 0\n",
    "    for i, trajs in enumerate(trajs_vec):\n",
    "        steps = trajs.shape[1]\n",
    "        opt_results[avg_terms_list[i]] = [mean_squared_error(\n",
    "            trajs.flatten(),\n",
    "            adjusted_preds[start:start + steps, ...].transpose(1,0,2).flatten(),\n",
    "            squared=False)]\n",
    "        # if i == 0:\n",
    "        #     fig, ax = plt.subplots()\n",
    "        #     ax.plot(trajs[0].flatten())\n",
    "        #     ax.plot(adjusted_preds[start:start + steps, 0, :].flatten())\n",
    "        #     print(adjusted_preds[start:start + steps, 0, :].flatten())\n",
    "        start += steps\n",
    "\n",
    "    opt_results = pd.DataFrame(opt_results)\n",
    "    all_opt_results.append(opt_results)\n",
    "\n",
    "all_opt_results = pd.concat(all_opt_results, axis=0)\n",
    "all_opt_results = all_opt_results.groupby(all_opt_results.index).agg(\n",
    "    [\"mean\", \"std\"])\n",
    "all_opt_results = all_opt_results.sort_index(axis=1)\n",
    "all_opt_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "batch_n = 1\n",
    "\n",
    "# Load\n",
    "pth = \"results/mfred/example-20230815-052223\"\n",
    "\n",
    "# Wind\n",
    "# pth = \"results/nrel/example-20230819-070441\"\n",
    "\n",
    "# 13, 7\n",
    "# sample_choose = 93\n",
    "rand_seed = np.random.randint(0,1000)\n",
    "sample_choose = np.random.permutation(128)[10]\n",
    "\n",
    "with open(f\"{pth}-{seed}.pkl\", \"rb\") as f:\n",
    "    result = pickle.load(f)\n",
    "    model_hyperparams = result[\"model_hyperparams\"]\n",
    "    data_params = result[\"data_params\"]\n",
    "    model_state_dict = result[\"Hierarchical NL\"][\"model_state_dict\"]\n",
    "from model import GeneralHNL\n",
    "from dataset import generate_data_set, setup_seed\n",
    "\n",
    "setup_seed(rand_seed)\n",
    "\n",
    "(input_dim, output_dim, sample_rate, t, dltrain, dlval, dltest,\n",
    " input_timesteps, output_timesteps, train_mean, train_std,\n",
    " feature) = generate_data_set(**data_params)\n",
    "m = GeneralHNL(**model_hyperparams).to(data_params[\"device\"])\n",
    "m.model.load_state_dict(model_state_dict)\n",
    "\n",
    "train_mean, train_std = train_mean.cpu().numpy()[\n",
    "    feature[\"fcst_feature\"]], train_std.cpu().numpy()[feature[\"fcst_feature\"]]\n",
    "count = 0\n",
    "for batch in dltest:\n",
    "    if count == batch_n:\n",
    "        break\n",
    "\n",
    "    count += 1\n",
    "\n",
    "labels = []\n",
    "for avg_terms in [12, 3, 1]:\n",
    "    data_to_predict = batch[\"data_to_predict\"].transpose(1, 2)\n",
    "    data_to_predict = torch.nn.functional.avg_pool1d(data_to_predict,\n",
    "                                                     avg_terms, avg_terms)\n",
    "    data_to_predict = data_to_predict.transpose(1, 2)\n",
    "    labels.append(data_to_predict.detach().cpu().numpy())\n",
    "\n",
    "all_decomp_fcsts, _ = m.model.decompose_predcit(batch[\"observed_data\"],\n",
    "                                                batch[\"available_forecasts\"],\n",
    "                                                batch[\"observed_tp\"],\n",
    "                                                batch[\"tp_to_predict\"])\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=[15, 6])\n",
    "fig2, axs2 = plt.subplots(3, figsize=[5, 6])\n",
    "for i in range(len(all_decomp_fcsts)):\n",
    "    sub_comps = all_decomp_fcsts[i]\n",
    "    # print(len(sub_comps))\n",
    "    fcsts = []\n",
    "    for j in range(len(sub_comps)):\n",
    "        comps = sub_comps[j]\n",
    "        # print(comps)\n",
    "        comps_plt = comps[sample_choose].squeeze(\n",
    "        ) * train_std + train_mean if j == 0 else comps[sample_choose].squeeze(\n",
    "        ) * train_std\n",
    "        comps_plt = np.where(comps_plt > 16, 16, comps_plt)\n",
    "\n",
    "        axs[i, j].plot(comps_plt, lw=2, c=plt.get_cmap(\"Set2\").colors[j])\n",
    "        axs[i, j].grid(alpha=0.5)\n",
    "        axs[i, j].set_axisbelow(True)\n",
    "        axs[i, j].set_axisbelow(True)\n",
    "        if j + 1 >= 2:\n",
    "            fcsts.append(comps[sample_choose].squeeze())\n",
    "        else:\n",
    "            fcsts.append(comps[sample_choose].squeeze())\n",
    "\n",
    "    plt_fcst = np.sum(fcsts, axis=0) * train_std + train_mean\n",
    "    plt_fcst = np.where(plt_fcst < 0, 0, plt_fcst)\n",
    "    plt_fcst = np.where(plt_fcst > 16, 16, plt_fcst)\n",
    "    axs2[i].plot(plt_fcst, lw=2, c=\"grey\", label=\"predict\")\n",
    "    # axs2[i].plot(labels[i][sample_choose] * train_std + train_mean,\n",
    "    #              lw=2,\n",
    "    #              c=\"grey\",\n",
    "    #              ls=\"--\",\n",
    "    #              label=\"real\")\n",
    "    axs2[i].grid(alpha=0.5)\n",
    "\n",
    "    axs2[i].set_axisbelow(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig2.tight_layout()\n",
    "fig.delaxes(axs[0, 1])\n",
    "fig.delaxes(axs[0, 2])\n",
    "fig.delaxes(axs[1, 2])\n",
    "\n",
    "axs2[-1].set_xlabel(\"Assembled forecasts\", fontweight=\"bold\")\n",
    "axs[-1, 0].set_xlabel(\"Component 1\", fontweight=\"bold\")\n",
    "axs[-1, 1].set_xlabel(\"Component 2\", fontweight=\"bold\")\n",
    "axs[-1, 2].set_xlabel(\"Component 3\", fontweight=\"bold\")\n",
    "axs[0, 0].set_ylabel(\"60min\", fontweight=\"bold\")\n",
    "axs[1, 0].set_ylabel(\"15min\", fontweight=\"bold\")\n",
    "axs[2, 0].set_ylabel(\"5min\", fontweight=\"bold\")\n",
    "axs2[0].set_ylabel(\"60min\", fontweight=\"bold\")\n",
    "axs2[1].set_ylabel(\"15min\", fontweight=\"bold\")\n",
    "axs2[2].set_ylabel(\"5min\", fontweight=\"bold\")\n",
    "axs2[0].legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
